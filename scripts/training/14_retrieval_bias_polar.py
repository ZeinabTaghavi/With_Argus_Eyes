"""
Retrieval Bias Polar Plot and Metrics
------------------------------------

This script produces comparative visualisations and summary metrics
from the per-retriever bias statistics generated by
``13_retrieval_bias_analysis.py``.  After running the analysis script
for each retriever, JSON files will be present in a single
directory—typically ``outputs/11_bias_statistics``.  Each
file contains per-category scores for the HateXplain and CrowS-Pairs
datasets.  The present script reads all JSON files in the input
directory, computes category-wise differences between biased and
neutral sentences for HateXplain and stereotypical versus anti
sentences for CrowS-Pairs, and visualises these differences on polar
charts.

For each dataset and retriever:

* **HateXplain:** the difference value for a target community
  ``c`` is computed as

  ``mean_biased(c) – mean_neutral(c)``

  where ``mean_biased`` and ``mean_neutral`` are taken from the
  ``'biased'`` and ``'neutral'`` fields in the stats JSON.  Larger
  positive differences indicate that the model assigns higher risk to
  hate/offensive posts relative to normal posts for that community.

* **CrowS-Pairs:** the difference value for a bias type ``c`` is
  simply the ``'difference'`` mean (stereotypical minus
  anti-stereotypical) stored in the JSON.  Larger values indicate
  higher risk for stereotypical sentences.

The script builds a single polar plot per dataset where each
retriever is represented as a radial polygon connecting its
per-category differences.  Only categories that appear across all
retrievers are plotted by default (intersection); you may choose to
plot the union via the ``--union`` flag.  Summary metrics are
computed for each retriever and written to a JSON file; these include
the average difference across categories per dataset as well as the
difference of these averages between datasets.

Usage example:

.. code:: bash

    python 14_retrieval_bias_polar.py \
        --input_dir outputs/11_bias_statistics \
        --output_dir outputs/12_bias_plots \
        --union

By default the script uses the intersection of categories across
retrievers and writes results to ``outputs/12_bias_plots``.
"""

import argparse
import json
import os
from typing import Dict, List, Tuple

import numpy as np
import matplotlib.pyplot as plt

###############################################################################
# CrowS-Pairs bias label mapping
#
# In your JSON, CrowS-Pairs bias types appear as numeric codes ("0".."8").
# The original dataset also defines string labels like "race/color",
# "gender/gender identity", etc.  Here we map BOTH forms to readable labels.
###############################################################################

CROWS_BIAS_LABEL_MAP: Dict[str, str] = {
    # Numeric codes (as stored in JSON keys)
    "0": "Race / Color",
    "1": "Gender / Identity",
    "2": "Sexual Orientation",
    "3": "Religion",
    "4": "Age",
    "5": "Nationality",
    "6": "Disability",
    "7": "Physical Appearance",
    "8": "Socioeconomic Status",
    # Also accept textual keys if they ever appear
    "race/color": "Race / Color",
    "gender/gender identity": "Gender / Identity",
    "sexual orientation": "Sexual Orientation",
    "religion": "Religion",
    "age": "Age",
    "nationality": "Nationality",
    "disability": "Disability",
    "physical appearance": "Physical Appearance",
    "socioeconomic status": "Socioeconomic Status",
}


def load_json_results(
    input_dir: str,
) -> Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]:
    """Load all JSON result files in a directory.

    Each file is expected to contain a dictionary with keys
    'HateXplain' and 'CrowS_Pairs', mapping to per-category statistics.
    The file name (without extension) is treated as the retriever name.
    """
    results: Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]] = {}
    for fname in os.listdir(input_dir):
        if not fname.lower().endswith(".json"):
            continue
        retriever = os.path.splitext(fname)[0]
        path = os.path.join(input_dir, fname)
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if "HateXplain" not in data or "CrowS_Pairs" not in data:
            continue
        results[retriever] = data
    return results


def compute_differences(
    results: Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]
) -> Tuple[Dict[str, Dict[str, Dict[str, float]]], Dict[str, Dict[str, Dict[str, float]]]]:
    """Compute category difference values for HateXplain and CrowS-Pairs."""

    hate_diffs: Dict[str, Dict[str, float]] = {}
    crows_diffs: Dict[str, Dict[str, float]] = {}

    for retriever, data in results.items():
        # HateXplain
        hate_data = data.get("HateXplain", {})
        hate_diff: Dict[str, float] = {}
        for cat, stats in hate_data.items():
            biased = stats.get("biased") or {}
            neutral = stats.get("neutral") or {}
            if biased.get("count", 0) == 0 or neutral.get("count", 0) == 0:
                continue
            diff_val = float(biased.get("mean", 0.0)) - float(neutral.get("mean", 0.0))
            hate_diff[cat] = diff_val
        hate_diffs[retriever] = hate_diff

        # CrowS-Pairs
        crows_data = data.get("CrowS_Pairs", {})
        crows_diff: Dict[str, float] = {}
        for cat, stats in crows_data.items():
            diff_stats = stats.get("difference") or {}
            if "mean" in diff_stats:
                crows_diff_val = float(diff_stats["mean"])
            else:
                stereo = stats.get("stereotypical") or {}
                anti = stats.get("anti") or {}
                if stereo.get("count", 0) == 0 or anti.get("count", 0) == 0:
                    continue
                crows_diff_val = float(stereo.get("mean", 0.0)) - float(
                    anti.get("mean", 0.0)
                )
            crows_diff[cat] = crows_diff_val
        crows_diffs[retriever] = crows_diff

    return hate_diffs, crows_diffs


def determine_categories(
    diffs: Dict[str, Dict[str, float]], use_union: bool = False
) -> List[str]:
    """Determine the set of categories across retrievers."""

    categories_sets = [set(d.keys()) for d in diffs.values() if d]
    if not categories_sets:
        return []
    if use_union:
        cat_set = set().union(*categories_sets)
    else:
        cat_set = set(categories_sets[0])
        for s in categories_sets[1:]:
            cat_set &= s
    return sorted(cat_set)


def prepare_plot_data(
    categories: List[str], diffs: Dict[str, Dict[str, float]]
) -> Dict[str, List[float]]:
    """Prepare per-retriever value lists aligned to categories."""

    plot_data: Dict[str, List[float]] = {}
    for retriever, cat_vals in diffs.items():
        vals: List[float] = []
        for cat in categories:
            vals.append(cat_vals.get(cat, 0.0))
        plot_data[retriever] = vals
    return plot_data


# -----------------------------------------------------------------------------


def downsample_for_plot(
    categories: List[str],
    plot_data: Dict[str, List[float]],
    stride: int,
) -> Tuple[List[str], Dict[str, List[float]]]:
    """Downsample categories and aligned plot data by a fixed stride."""

    if stride is None or stride <= 1 or not categories:
        return categories, plot_data
    indices = list(range(0, len(categories), stride))
    new_categories = [categories[i] for i in indices]
    new_plot_data: Dict[str, List[float]] = {}
    for retriever, vals in plot_data.items():
        if not vals:
            new_plot_data[retriever] = []
            continue
        new_plot_data[retriever] = [vals[i] for i in indices if i < len(vals)]
    return new_categories, new_plot_data


def plot_polar_chart(
    dataset_name: str,
    categories: List[str],
    plot_data: Dict[str, List[float]],
    output_path: str,
    *,
    title_suffix: str = "Bias Difference",
    category_label_map: Dict[str, str] | None = None,
) -> None:
    """Create and save a polar plot for the given dataset."""

    if not categories:
        return

    num_cats = len(categories)
    angles = np.linspace(0, 2 * np.pi, num_cats, endpoint=False).tolist()
    angles += angles[:1]

    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection="polar"))

    for retriever, vals in plot_data.items():
        if not vals:
            continue
        vals_closed = vals + vals[:1]
        ax.plot(angles, vals_closed, label=retriever)
        ax.fill(angles, vals_closed, alpha=0.1)

    # Map raw keys to readable labels if provided
    if category_label_map:
        theta_labels = [category_label_map.get(str(cat), str(cat)) for cat in categories]
    else:
        theta_labels = [str(cat) for cat in categories]

    # NOTE: older Matplotlib doesn't support `frac` kwarg here, so we just
    # use the standard signature and push labels out via tick_params pad.
    ax.set_thetagrids(np.degrees(angles[:-1]), theta_labels)
    ax.set_title(f"{dataset_name}: {title_suffix}", va="bottom")
    ax.set_rlabel_position(0)

    all_vals = [v for vals in plot_data.values() for v in vals]
    if all_vals:
        max_abs = max(abs(v) for v in all_vals)
        ax.set_ylim(-max_abs * 1.1, max_abs * 1.1)

    # Make angular labels a bit farther from center and slightly smaller
    ax.tick_params(axis="x", labelsize=8, pad=25)
    ax.tick_params(axis="y", labelsize=8)
    ax.legend(loc="upper right", bbox_to_anchor=(1.1, 1.1))
    fig.tight_layout()
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fig.savefig(output_path)
    plt.close(fig)


def compute_metrics(
    categories: List[str],
    diffs: Dict[str, List[float]],
    threshold_diffs: Dict[str, List[float]],
) -> Dict[str, Dict[str, float]]:
    """Compute summary metrics for each retriever."""

    metrics: Dict[str, Dict[str, float]] = {}
    for retriever, vals in diffs.items():
        numeric_vals = [v for v in vals if v is not None]
        mean_diff = float(np.mean(numeric_vals)) if numeric_vals else 0.0
        met = {"mean_difference": mean_diff}
        tvals = threshold_diffs.get(retriever)
        if tvals:
            numeric_tvals = [v for v in tvals if v is not None]
            met["mean_threshold_difference"] = (
                float(np.mean(numeric_tvals)) if numeric_tvals else 0.0
            )
        metrics[retriever] = met
    return metrics


def compute_threshold_differences(
    results: Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]],
    dataset_name: str,
) -> Dict[str, Dict[str, float]]:
    """Compute per-category threshold difference values."""

    th_diffs: Dict[str, Dict[str, float]] = {}
    for retriever, data in results.items():
        dataset_stats = data.get(dataset_name, {})
        cat_diffs: Dict[str, float] = {}
        for cat, stats in dataset_stats.items():
            if dataset_name == "HateXplain":
                biased = stats.get("biased") or {}
                neutral = stats.get("neutral") or {}
                b_count = float(biased.get("count", 0))
                n_count = float(neutral.get("count", 0))
                if b_count > 0 and n_count > 0:
                    b_above = float(biased.get("above_threshold", 0)) / b_count
                    n_above = float(neutral.get("above_threshold", 0)) / n_count
                    cat_diffs[cat] = b_above - n_above
            elif dataset_name == "CrowS_Pairs":
                stereo = stats.get("stereotypical") or {}
                anti = stats.get("anti") or {}
                s_count = float(stereo.get("count", 0))
                a_count = float(anti.get("count", 0))
                if s_count > 0 and a_count > 0:
                    s_above = float(stereo.get("above_threshold", 0)) / s_count
                    a_above = float(anti.get("above_threshold", 0)) / a_count
                    cat_diffs[cat] = s_above - a_above
        th_diffs[retriever] = cat_diffs
    return th_diffs


def save_metrics_json(metrics: Dict[str, Dict[str, float]], path: str) -> None:
    """Save metrics dictionary to a JSON file."""

    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Plot polar charts and compute bias metrics from analysis results."
    )
    parser.add_argument(
        "--input_dir",
        type=str,
        required=True,
        help="Directory containing the per-retriever JSON results from 13_retrieval_bias_analysis.py",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        required=True,
        help="Directory where plots and metrics will be saved.",
    )
    parser.add_argument(
        "--union",
        action="store_true",
        help="Use the union of categories across retrievers instead of the intersection.",
    )
    parser.add_argument(
        "--hatexplain_stride",
        type=int,
        default=1,
        help=(
            "Stride for downsampling HateXplain categories on the polar plot "
            "(default: 1, i.e., show all categories)."
        ),
    )
    parser.add_argument(
        "--crows_stride",
        type=int,
        default=1,
        help=(
            "Stride for downsampling CrowS-Pairs categories on the polar plot "
            "(default: 1, i.e., show all categories)."
        ),
    )
    args = parser.parse_args()

    input_dir = args.input_dir
    output_dir = args.output_dir
    use_union = args.union
    hate_stride = args.hatexplain_stride
    crows_stride = args.crows_stride

    results = load_json_results(input_dir)
    if not results:
        print(f"No valid JSON results found in {input_dir}")
        return

    hate_diffs, crows_diffs = compute_differences(results)
    hate_th_diffs = compute_threshold_differences(results, "HateXplain")
    crows_th_diffs = compute_threshold_differences(results, "CrowS_Pairs")

    hate_categories = determine_categories(hate_diffs, use_union)
    crows_categories = determine_categories(crows_diffs, use_union)

    hate_plot_data_full = prepare_plot_data(hate_categories, hate_diffs)
    crows_plot_data_full = prepare_plot_data(crows_categories, crows_diffs)

    hate_plot_categories, hate_plot_data = downsample_for_plot(
        hate_categories, hate_plot_data_full, hate_stride
    )
    crows_plot_categories, crows_plot_data = downsample_for_plot(
        crows_categories, crows_plot_data_full, crows_stride
    )

    if hate_plot_categories:
        hate_plot_path = os.path.join(output_dir, "HateXplain_polar.png")
        plot_polar_chart("HateXplain", hate_plot_categories, hate_plot_data, hate_plot_path)
        print(f"Saved HateXplain polar plot to {hate_plot_path}")
    else:
        print("No common categories found for HateXplain; skipping plot.")

    if crows_plot_categories:
        crows_plot_path = os.path.join(output_dir, "CrowS_Pairs_polar.png")
        plot_polar_chart(
            "CrowS_Pairs",
            crows_plot_categories,
            crows_plot_data,
            crows_plot_path,
            category_label_map=CROWS_BIAS_LABEL_MAP,
        )
        print(f"Saved CrowS-Pairs polar plot to {crows_plot_path}")
    else:
        print("No common categories found for CrowS_Pairs; skipping plot.")

    hate_metrics = (
        compute_metrics(
            hate_categories,
            hate_plot_data_full,
            prepare_plot_data(hate_categories, hate_th_diffs),
        )
        if hate_categories
        else {}
    )
    crows_metrics = (
        compute_metrics(
            crows_categories,
            crows_plot_data_full,
            prepare_plot_data(crows_categories, crows_th_diffs),
        )
        if crows_categories
        else {}
    )

    combined_metrics: Dict[str, Dict[str, float]] = {}
    for retriever in results.keys():
        metrics_entry: Dict[str, float] = {}
        if retriever in hate_metrics:
            for key, val in hate_metrics[retriever].items():
                metrics_entry[f"HateXplain_{key}"] = val
        if retriever in crows_metrics:
            for key, val in crows_metrics[retriever].items():
                metrics_entry[f"CrowS_Pairs_{key}"] = val
        h_mean = metrics_entry.get("HateXplain_mean_difference")
        c_mean = metrics_entry.get("CrowS_Pairs_mean_difference")
        if h_mean is not None and c_mean is not None:
            metrics_entry["Difference_between_datasets"] = h_mean - c_mean
        combined_metrics[retriever] = metrics_entry

    metrics_path = os.path.join(output_dir, "bias_metrics.json")
    save_metrics_json(combined_metrics, metrics_path)
    print(f"Saved summary metrics to {metrics_path}")


if __name__ == "__main__":
    main()
